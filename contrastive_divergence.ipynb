{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "In this project, we will look into Contrastive Divergence method of training an energy based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "For the purpose of this project we will use the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "loader = DataLoader(mnist, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBM\n",
    "Here we will use a neural network as the energy function. The energy function will output a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Divergence Training\n",
    "Remember that for getting $\\nabla_{\\theta} \\log(p_{\\theta})$, we need to compute:\n",
    "$\\nabla_{\\theta} \\log(p_{\\theta}(x)) = -\\nabla_{\\theta}E_{\\theta}(x) + \\mathbb{E}_{x' \\sim p_{\\theta}}[\\nabla_{\\theta}E_{\\theta}(x')]$\n",
    "\n",
    "$x'$ is calculated by using MCMC\n",
    "\n",
    "$x^{k+1} = x^{k} + \\frac{\\epsilon^2} {2}   \\nabla_{\\theta} \\log(p_{\\theta}(x)) + \\epsilon z^k$\n",
    "\n",
    "So we have positive phase: Minimize $E_{\\theta}(x_{data})$ and negative phase: $E_{\\theta}(x_{model})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mcmc_samples(x_init, ebm_model, num_steps=20, epsilon=0.1):\n",
    "    x_neg = x_init.clone().detach().to(device)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        x_neg.requires_grad = True\n",
    "        energy = ebm_model(x_neg) \n",
    "        grad = torch.autograd.grad(energy.sum(), x_neg)[0]\n",
    "        x_neg = x_neg - 0.5 * epsilon**2 * grad + epsilon * torch.randn_like(x_neg)\n",
    "        \n",
    "        x_neg = x_neg.detach()\n",
    "        \n",
    "    return x_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a simple visualization script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_visualize(model, steps=100, epsilon=0.1, n_samples=16):\n",
    "    model.eval()\n",
    "\n",
    "    # Step 1: Start from pure noise\n",
    "    x_init = torch.rand((n_samples, 1, 28, 28)).to(device)\n",
    "\n",
    "    # Step 2: Run long-run Langevin sampling\n",
    "    x_sampled = get_mcmc_samples(x_init, model, num_steps=steps, epsilon=epsilon)\n",
    "\n",
    "    # Step 3: Plot a grid of samples\n",
    "    x_sampled = x_sampled.cpu().detach().clamp(0, 1)  # Clip values to [0, 1] for display\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(x_sampled[i][0], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    model.train()  # Go back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EBM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for x_data, _ in loader:\n",
    "        x_data = x_data.to(device)\n",
    "\n",
    "        # Step 1: Initialize x_neg from random noise or uniform image\n",
    "        x_init = torch.rand_like(x_data)  # or torch.randn_like(x_data)\n",
    "        x_neg = get_mcmc_samples(x_init, model, num_steps=20, epsilon=0.1)\n",
    "\n",
    "        # Step 2: Compute energies\n",
    "        energy_pos = model(x_data)       # E(x_data)\n",
    "        energy_neg = model(x_neg)        # E(x_neg)\n",
    "\n",
    "        # Step 3: Contrastive divergence loss\n",
    "        loss = energy_pos.mean() - energy_neg.mean()\n",
    "\n",
    "        # Step 4: Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"Sampling from model...\")\n",
    "    sample_and_visualize(model, steps=100, epsilon=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
