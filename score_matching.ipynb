{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "In this project, we will look into Score Matching method of training an energy based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyCIFAR10(torch.utils.data.Dataset):\n",
    "    def __init__(self, sigma_levels, train=True):\n",
    "        super().__init__()\n",
    "        self.data = datasets.CIFAR10(\n",
    "            root=\"./data\",\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform=transforms.ToTensor()\n",
    "        )\n",
    "        self.sigma_levels = torch.tensor(sigma_levels).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = self.data[idx]\n",
    "\n",
    "        sigma = self.sigma_levels[torch.randint(0, len(self.sigma_levels), (1,))].item()\n",
    "\n",
    "        noise = torch.randn_like(x) * sigma\n",
    "        x_noisy = x + noise\n",
    "\n",
    "        return x, x_noisy, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFourierProjection(torch.nn.Module):\n",
    "    \"\"\"Encode log(sigma) using random Fourier features.\"\"\"\n",
    "    def __init__(self, embedding_size=128, scale=1.0):\n",
    "        super().__init__()\n",
    "        self.W = torch.randn(embedding_size // 2, dtype=torch.float32) * scale\n",
    "\n",
    "    def forward(self, sigma):\n",
    "        # sigma: [B]\n",
    "        sigma = sigma.view(-1, 1).float()  # Explicit float32 conversion\n",
    "        x_proj = sigma * self.W.to(sigma.device) * 2 * math.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNet(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        self.embedding = GaussianFourierProjection(embedding_size)\n",
    "\n",
    "        self.cond_proj = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3 + 1, 64, 3, padding=1),  # 3 image channels + 1 noise channel\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_noisy, sigma):\n",
    "        emb = self.embedding(torch.log(sigma))\n",
    "        cond = self.cond_proj(emb)\n",
    "\n",
    "        cond = cond.view(-1, 128, 1, 1).expand(-1, 128, x_noisy.shape[2], x_noisy.shape[3])\n",
    "\n",
    "        sigma_map = sigma.view(-1, 1, 1, 1).expand(-1, 1, x_noisy.shape[2], x_noisy.shape[3])\n",
    "        input = torch.cat([x_noisy, sigma_map], dim=1)  # [B, 4, H, W]\n",
    "\n",
    "        return self.net(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsm_loss(score_model, x, x_noisy, sigma):\n",
    "    \"\"\"\n",
    "    x: clean image [B, 3, H, W]\n",
    "    x_noisy: noisy image [B, 3, H, W]\n",
    "    sigma: noise level [B]\n",
    "    \"\"\"\n",
    "    # Ensure all inputs are float32\n",
    "    x = x.float()\n",
    "    x_noisy = x_noisy.float()\n",
    "    sigma = sigma.float()\n",
    "\n",
    "    z = (x_noisy - x) / sigma.view(-1, 1, 1, 1)  # [B, 3, 32, 32]\n",
    "\n",
    "    # Model prediction\n",
    "    score_pred = score_model(x_noisy, sigma)\n",
    "\n",
    "    target = -z\n",
    "    \n",
    "    loss = ((score_pred - target) ** 2).sum(dim=(1, 2, 3))  # per-sample loss\n",
    "    return loss.mean()  # average over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def annealed_langevin_sample(score_model, sigmas, num_steps=20, step_size=0.01, shape=(16, 3, 32, 32)):\n",
    "    score_model.eval()\n",
    "    x = torch.randn(*shape).to(device)\n",
    "\n",
    "    for sigma in reversed(sigmas):  # large noise â†’ small noise\n",
    "        sigma = torch.tensor([sigma] * shape[0], device=device)\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            x.requires_grad = True\n",
    "            score = score_model(x, sigma)\n",
    "            x = x + 0.5 * step_size**2 * score\n",
    "            x = x + step_size * torch.randn_like(x)\n",
    "            x = x.detach()\n",
    "\n",
    "    return x.clamp(0, 1)  # Clamp to [0, 1] range for display\n",
    "\n",
    "\n",
    "def show_samples(x, nrow=4):\n",
    "    x = x.cpu()\n",
    "    fig, axes = plt.subplots(nrow, nrow, figsize=(6, 6))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = x[i].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2628046/3360474217.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.sigma_levels = torch.tensor(sigma_levels).float()\n"
     ]
    }
   ],
   "source": [
    "SIGMA_LEVELS = torch.exp(torch.linspace(math.log(0.01), math.log(1.0), 10))\n",
    "num_epochs = 5000\n",
    "\n",
    "train_dataset = NoisyCIFAR10(sigma_levels=SIGMA_LEVELS, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: DSM Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ScoreNet().to(device)\n",
    "# Make sure all model parameters are float32\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.float()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for x, x_noisy, sigma in train_loader:\n",
    "        x = x.to(device).float()  # Ensure float32\n",
    "        x_noisy = x_noisy.to(device).float()  # Ensure float32\n",
    "        sigma = sigma.to(device).float()  # Ensure float32\n",
    "\n",
    "        # 1. Forward pass + DSM loss\n",
    "        loss = dsm_loss(model, x, x_noisy, sigma)\n",
    "\n",
    "        # 2. Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}: DSM Loss = {avg_loss:.4f}\")\n",
    "        samples = annealed_langevin_sample(model, sigmas=SIGMA_LEVELS, shape=(16, 3, 32, 32))\n",
    "        show_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
